% Created 2016-07-14 jue 01:07
\documentclass[11pt,letterpaper]{article}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{xltxtra}
\usepackage[top=2.0cm, bottom=3cm, left=2.0cm, right=2.0cm]{geometry}
\author{Juan Ávalo, Bastien Got}
\date{\today}
\title{Tarea 3 Análisis Inteligente de Datos}
\hypersetup{
 pdfauthor={Juan Ávalo, Bastien Got},
 pdftitle={Tarea 3 Análisis Inteligente de Datos},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.1.50.2 (Org mode 8.3.4)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Reducción de Dimensionalidad para Clasificación}
\label{sec:orgheadline1}
La idea del problema es de clasificar las once vocales del inglés británico
para poder reconocer éstas vocales independamente de la persona que habla. Los
cadidatos dijeron 11 palabras cada uno, que corresponden a las once vocales.
Luego, la clasificación será probada con nuevos candidatos.

\begin{enumerate}
\item En el caso de los datos de entrenamiento, hay 48 registros que corresponden
al numero de veces distintas que cada palabra fue registrada. Hay 42
registros en el dataset de prueba. Por cada registro, la persona dijo 11
palabras que corresponden a las 11 vocales de la idioma.
\item Normalizamos los datos para que una de las variables no tenga un mayor peso
que las otras solo por tener valores mas grandes en valor absoluto.
\item Elección de paleta : \textbf{Espectral}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{p1_spectral.PNG}
\end{figure} 
Porque es la paleta que tiene la divesidad de colores más grande y por lo
tanto es más fácil de ver donde se situan los datos de cada clase.

En el codigo dado en la tarea mclases tiene 9 elementos, sin embargo el
numero de clases es 11 por el numero de vocales.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{p1_C_PCA_PLOT.png}
\caption{\emph{PCA}.}
\end{figure}
\item La paleta es la misma que en la pregunta anterior.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{p1_D_LDA_PLOT.png}
\caption{\emph{LDA}.}
\end{figure}
\item Cualitativamente parece que el metodo de reducción de dimensionalidad LDA
es más interesante que el PCA ya que las clases son más separadas con LDA,
o dicho de otra manera por cada clase los datos son más agrupados usando LDA.

Podemos verificarlo utilizando el metodo del K-means: se debe calcular la
distancia de cada punto con el promedio de los puntos de su clase. El
metodo más adecuado es el que minimiza la suma de los cuadros de estas
distancias
\item Cada palabra es dicha el mismo numero de veces : 48 por los datos de
pruebas. Lo que significa que una palabra tiene la misma probabilidad a
priori de pertenecer a cada una de las clases : 1/11 ≃ 0.091.
\item La tabla siguiente muestra la puntuación de cada modelo sobre los datos de
entrenamiento y los datos de prueba.

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Modelo & LDA & QDA & 10-NeighborsClass\\
\hline
Entrenamiento & 0.68 & \textbf{0.98} & 0.93\\
Prueba & 0.45 & 0.42 & \textbf{0.49}\\
\hline
\end{tabular}
\end{center}

El modelo más apropriado para los datos de entrenamiento es QDA con una
puntuación de 0.98. El modelo que se comporta lo mejor sobre los datos de
prueba es el K-NeighborsClasifier con \(K = 10\) que nos da una puntuación de
0.49.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{p1_KNeighbors_score.png}
\caption{Score de K-Neighbors.}
\end{figure}
El score de entrenamiento va bajando cuando K aumenta, aunque el score de
prueba se queda entre 0.45 y 0.5. El mejor que podemos obtener es un score
de prueba de 0.498 con \(K = 1\) y \(K = 8\).

\item En esta pregunta utilizamos PCA para reducir la dimensionalidad. El mejor
modelo es el que minimiza el error de prueba y por lo tanto encontramos que
QDA en aplicado al dataset reducido a una dimensión es lo más interesante,
con un error de prueba de 0.77.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{p1_Dim_Error_PCA.png}
\caption{Errores de entrenamiento LDA, QDA, K-NN usando PCA.}
\end{figure}
\item Utilizando LDA para reducir la dimensionalidad del dataset, encontramos que
el modelo más interesante todavía es QDA, pero esta vez con un error de
prueba de 0.78.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{p1_Dim_Error_LDA.png}
\caption{Errores de entrenamiento LDA, QDA, K-NN usando LDA.}
\end{figure}
\end{enumerate}


\section{Análisis de Opiniones sobre Películas.}
\label{sec:orgheadline4}
\subsection{Preprocesamiento de los datos.}
\label{sec:orgheadline2}
\begin{enumerate}
\item Tanto el set de entrenamiento como el de pruebas tienen \textbf{3554} datos.
Los datos se separan en textos positivos y negativos de acuerdo a la tabla:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Sets & Positivos & Negativos\\
\hline
Entrenamiento & 1770 & 1784\\
Prueba & 1803 & 1751\\
\hline
\end{tabular}
\end{center}
\item Se creó una función que extrae palabras usando \emph{stemming} y quitando
\emph{stopwords} si así es pedido. Sobre los resultados obtenidos se puede
observar con los ejemplos usados que \emph{stemming} lo que hace es hacer que
una palabra tome una pseudoraiz. Además, el proceso de quitar \emph{stopwords}
efectivamente quita palabras como "I" o "to".

Como ejemplo, las frases: "I love to eat cake" y "I love eating cake" ambas
se reducen a "love", "eat" y "cake".

Pero un ejemplo más interesante es el de aplicar \emph{stemming} sobre palabras
como "absolutely" y "dislike", las cuales se traducen en "absolut" y
"dislik". Ninguna de las pseudoraices es una palabra del inglés verdadera,
pero van a ser útiles para saber que palabras están relacionadas o no.
\item También se creó una función análoga a la del punto anterior, pero
lematizando. Para poder lograr ésto no se pudo usar la función
\texttt{WordNetLemmatizer} directamente como estaba en los ejemplos.

Lematización involucra hacer un análisis sintáctico de las palabras, por lo
que la función usada pide marcar cada palabra con la posición dentro de la
oración que tiene (verbo, sustantivo, adjetivo, adverbio), la cual se
obtuvo mediante la función \texttt{pos\_tag}.

Los efectos en las palabras de ejemplo son aparentes. En casos como el de
"I love to eat cake" el lematizador las reduce de la misma forma que usando
\emph{stemming}. La diferencia se nota al usar las palabras "dislike" y
"absolutely", las cuales se mantienen iguales. O con palabras como "are" e
"is" las cuales se reducen a "to be".
\item Se generaron cuatro representaciones vectoriales para los dos conjuntos de
datos. La razón de ésto es porque se necesita extraer palabras con
\emph{stemming} y con \emph{lemmatize}, con \emph{stopwords} y sin ellas.

La representación del texto consiste en resumir cada comentario a un vector
binario con todo el vocabulario obtenido de todos los mensajes. Si el
mensaje tiene una palabra dentro del vocabulario, el valor de la variable
correspondiente a esa palabra es \textbf{1}. Sino es \textbf{0}. 

Luego las etiquetas son \textbf{0} si el mensaje es negativo, y \textbf{1} si es
positivo.

Considerando como se tratan los datos, se puede rankear las palabras que
globálmente se encontraron por frecuencia. Un ejemplo de ello es la
siguiente tabla:

\begin{center}
\begin{tabular}{|c|c|}
\hline
Frecuencia & Palabra\\
\hline
115 & way\\
125 & get\\
127 & well\\
128 & much\\
129 & work\\
143 & even\\
143 & time\\
145 & comedy\\
163 & character\\
169 & good\\
176 & story\\
246 & one\\
254 & like\\
264 & make\\
481 & movie\\
573 & film\\
\hline
\end{tabular}
\end{center}
\item El evaluador de desempeño considera las siguientes medidas:
\begin{itemize}
\item La precisión del modelo sobre los datos de entrenamiento.
\item La precisión del modelo sobre los datos de prueba.
\item La \emph{precisión}, esto es, el porcentaje de datos bien clasificados dentro de
todos los datos clasificados.
\item El \emph{recall} el cual es el porcentaje de los datos seleccionados bien
clasificados dentro de los datos de su clase.
\item El \emph{f1-score}, el cual es la media armónica entre la precisión y el
recall.
\item El support, que cuenta cuantos datos de cada clase hay.
\end{itemize}
\end{enumerate}
\subsection{Modelos a probar.}
\label{sec:orgheadline3}
Al dataset procesado mediante las combinaciones de stemming, lemmatize y
stopword removal se le aplicaron cuatro modelos de clasificación: "Naive
Bayes", "Multinomial Naive Bayes", "Logistic Regression" y "Linear SVC". 

En general los resultados eran prácticamente todos con un f1-score mayor a
0.70. Se notó en general que, sobre el comportamiento de las predicciones,
éstas eran más certeras mientras más vocabulario asociado a textos favorables
o desfavorables tenían. 

Ninguno de los tipos de modelo se comporta bien con frases sarcásticas o con
críticas que tienen muchas palabras que en otro contexto serían positivas.

Casi todos los modelos entregaban, a parte de la predicción concreta, un par
de valores que corresponde a la probabilidad de que el texto sea positivo o
negativo. Si ambos valores se parecían, indica que el modelo no tiene certeza
de que clase de texto es. Por otro lado, si uno de los valores es muy grande
indica una casi absoluta certeza en el resultado.

Los resultados generales para todos los modelos son resumidos por el
siguiente gráfico, tomando como medida el mayor f1-score obtenido por cada
modelo.
\begin{figure}[htb]
\centering
\includegraphics[width=0.6\textwidth]{p2_general.png}
\caption{Resultados generales.}
\end{figure} 

Curiosamente, el modelo más simple con la menor cantidad de preprocesamiento
es la que resulto mejor evaluada usando la métrica de f1-score y los modelos
más complicados resultaron peor evaluados. 

Viendo más en detalle, los resultados para los distintos modelos se
encuentran en los siguientes gráficos. El resumen para todos ellos es que
las predicciones son mejores mientras menos preprocesamiento haya.

En el caso del uso de \emph{stopwords} se asume que las palabras que son omitidas
mediante éste paso, para efectos de clasificar sí son importantes. Dejar de
lado palabras como "I" puede cambiar el contexto de la frase y con esto
cambiar el sentimiento que se quería transmitir.

Sobre la lematización se puede atribuir los problemas a dificultades en
obtener los \emph{tags} semánticos apropiados para cada palabra, más el hecho de
que los comentarios de internet de por si pueden tener palabras que no están
en el diccionario de WordNetLemmatizer, por lo cual no descubriría relaciones
que stemming si encuentra. 

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{p2_f1_naive.png}
\caption{\emph{Naive Bayes}.}
\end{figure} 


\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{p2_f1_multinomial.png}
\caption{\emph{multinomial naive bayes}.}
\end{figure} 


\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{p2_f1_logit.png}
\caption{\emph{Logistical Regression}.}
\end{figure} 


\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{p2_svm_logit.png}
\caption{\emph{Linear SVM}.}
\end{figure} 

Hay que hacer ciertas observaciones al respecto del parámetro \texttt{C} de
\emph{Logistical Regression} y \emph{Linear SVM}. Los gráficos anteriores muestran
valores aproximados de los mejores parámetros que se pueden elegir, los
cuales son \(C_{logit} = 0.9\) y \(C_{svm} = 0.05\). 

Además, analizando los valores de las probabilidades de obtener un label en
cada texto escogido por la función de test, se puede ver en el caso de
\emph{Logistical Regression} que aumentar \texttt{C} hace que las predicciones sean mas
determinantes. O sea, el modelo resultante tiene menos dudas al momento de
decidir a que label le corresponde cada texto. Viceversa si \texttt{C} disminuye,
para efectos del modelo los textos parecen ser más ambiguos. 

Usando la función \emph{Linear SVM} no se puede saber si el modelo resultante
sigue la misma tendencia con las probabilidades, ya que no son entregadas por
la función correspondiente. Usando \emph{SVM} con un kernel lineal, lo cual
debería ser equivalente salvo detalles de implementación, permite encontrar
las probabilidades, las cuales siguen el mismo patrón.
\end{document}